<!DOCTYPE html><!-- saved from url=(0025)https://qinghonglin.github.io/ --><html xmlns="http://www.w3.org/1999/xhtml" xml:lang="en"><head>
	<meta http-equiv="Content-Type" content="text/html; charset=UTF-8">
	<link rel="shortcut icon" href="myIcon.ico">
	<meta name="google-site-verification" content="PcjE-PoDvp7KoKeZ5wE1g_BU8VI5wioTfiAgbIst__4">
	<meta name="keywords" content="Qinghong Lin">
	<meta name="description" content="Qinghong Lin's homepage">
	<!-- <link rel="icon" href="icon.ico" type="figures/emoji"> -->
	<!-- <link rel="shortcut icon" href="https://em-content.zobj.net/thumbs/120/apple/325/technologist-light-skin-tone_1f9d1-1f3fb-200d-1f4bb.png"> -->
	<!-- <link rel="shortcut icon" href="https://em-content.zobj.net/thumbs/120/apple/325/snowflake_2744-fe0f.png"> -->
	<link rel="shortcut icon" href="images/world.png">
	<link rel="stylesheet" href="css/jemdoc.css" type="text/css">
	<title>Kevin Qinghong Lin @ University of Oxford</title>
	<script async="" src="js/analytics.js"></script>
	<script type="text/javascript" async="" src="js/ga.js"></script>
	<script type="text/javascript">
		var _gaq = _gaq || [];
		_gaq.push(['_setAccount', 'UA-39824124-1']);
		_gaq.push(['_trackPageview']);
		(function () {
			var ga = document.createElement('script');
			ga.type = 'text/javascript';
			ga.async = true;
			ga.src = ('https:' == document.location.protocol ? 'https://ssl' : 'http://www') + '.google-analytics.com/ga.js';
			var s = document.getElementsByTagName('script')[0];
			s.parentNode.insertBefore(ga, s);
		})();
	</script>
</head>

<body>
	<div id="layout-content" style="margin-top:25px">
		<!-- <table> -->
		<table style="margin-bottom: -20px;">
			<tbody>
				<tr>
					<td width="670">
						<div id="toptitle">

							<!-- <h1>Kevin Qinghong Lin</h1> -->
							<h1><span style="font-family: 'Comic Sans MS', Georgia; color: #6495ED;">Kevin</span>
								Qinghong Lin</h1>
						</div>
						<!-- <h3>Ph.D. Student</h3> -->
						<h3>Postdoctoral Researcher</h3>
						<p>
							<a href="index_19.html">Torr Vision Group</a><br>
							<a href="index_20.html">University of Oxford</a><br>
							<br>
							Email: <u><a href="mailto:kevin.qh.lin@gmail.com">kevin.qh.lin [at] gmail.com</a></u>
						</p>
						<p>
							<a href="citations_2.html"><img src="images/google_scholar.png" height="30px"></a>
							<a href="QinghongLin.html"><img src="images/github.png" height="30px"></a>
							<a href="kevinqhlin.html"><img src="images/linkedin.png" height="30px"></a>
							<a href="KevinQHLin.html"><img src="images/x.png" height="30px"></a>
						</p>
					</td>
					<td>
						<img src="images/kevin_new.jpeg" border="0" width="250"><br>
						<!-- <img src="./figures/kevin.jpeg" border="0" width="250"><br> -->
						<!-- <p style="font-size: 10px; font-family: 'Comic Sans MS', Georgia; text-align: left;">Photo taken on <a href="https://en.wikipedia.org/wiki/Rottnest_Island">Rottnest Island</a>.</p> -->
					</td>
				</tr>
				<tr>
				</tr>
			</tbody>
		</table>

		<h2>Biography</h2>
		<!-- <img src="./figures/wordcloud.png" style="float: right; width: 280px; margin: 0 0 10px 10px;">		 -->
		<p>
		</p>
		<div style="text-align:justify">
			I am a Postdoctoral
			Researcher in <a href="index_19.html">University of Oxford</a>, working with <a href="citations_3.html">Prof. Philip
				Torr</a>.
			<p></p>
			I obtained my PhD from <a href="home.html">National University of
				Singapore</a> in three years, luckily advised
			by <a href="citations_4.html">Prof. Mike
				Shou</a>.
			<p></p>
			<p></p>
			I was fortunate to intern at Tencent / Meta AI / Meta Reality Labs / Microsoft Research.
			<p></p>
			<!-- I used to work on video understanding. I’m now focus on agentic, visual intelligence.
			<p></p> -->
			<p>My research focuses on building multi-modal agents from human experience and applying them for human life.
			<!-- This involves abilities
				like:</p>
			<ul>
				<li>
					Perception: video understanding (<a href="https://videomind.github.io/">VideoMind</a>, <a href="https://showlab.github.io/videollm-online/">VideoLLM-online</a>), video-language pretraining (<a href="https://arxiv.org/abs/2206.01670">EgoVLP</a>, 
					<a href="https://arxiv.org/abs/2307.16715">UniVTG</a>)
				</li>
				<li>
					Reasoning:
					unified multimodal model (<a href="https://github.com/showlab/Show-o">Show-o</a>), reinforcement learning (<a href="https://arxiv.org/abs/2505.16854">Think or Not</a>, <a href="https://arxiv.org/abs/2508.08189">RL in Vision</a>)
				<li>
					Interaction: computer-use agents (<a href="https://github.com/showlab/ShowUI">ShowUI</a>, <a href="https://uivision.github.io/">UI-Vision</a>), agents for scientist (<a href="https://github.com/Paper2Poster/Paper2Poster">Paper2Poster</a>, <a href="https://showlab.github.io/Paper2Video/">Paper2Video</a>), code intelligene (<a href="https://showlab.github.io/Code2Video/">Code2Video</a>, <a href="https://csu-jpg.github.io/VCode/">VCode</a>)
				</li> -->
			
			
			</p><p>I’m open to collaborations and discussions. Feel free to drop me an email.</p>
<div style="float: left; text-align: center;">

    <img src="api" style="max-width: 420px;">

<p style="text-align: left;">
    I am passionate about open-source!
</p>


</div>
<div style="clear: both;"></div>
			<h2>News</h2>
			<div style="max-height: 360px; overflow-y: auto">
				<ul>
					<li>
						2025 Nov: Happy be selected as <a href="fellows.html">DAAD AINeT fellow 2025</a>. 
					</li>
					<li>
						2025 Nov: Give a talk at <a href="25-11-05-MultimodalLargeModels.html">BMVA Symposium: Multimodal Large Models</a>.
					</li>
					<li>
						2025 Oct: Check out our newest works on creative video generation: <a href="Code2Video.html">Code2Video</a> and <a href="Paper2Video.html">Paper2Video</a>!
					</li>
					<li>
						2025 Sept: <a href="index.html">Paper2Poster</a>, <a href="2505.16854">Think or Not</a> got accepted by
						<a href="index_21.html">NeurIPS
							2025</a>.
					</li>
					<li>
						2025 July: <a href="index.html">Paper2Poster</a> is selected as an <a style="color:#FA8072">Oral</a> by
						<a href="39955.html">ICML
							Multi-Agent Systems
							workshop
							2025</a>.
					</li>
					<li>
						2025 July: <a href="2406.13719">GUI-Narrator</a> got accepted by <a href="index_22.html">ACM MM
							2025</a>.
					</li>
					<li>
						2025 Jun: Selected for <a href="CallForDoctoralConsortium.html" style="color:#FA8072;">CVPR 2025 Doctoral Consortium</a>, Thank you!
					</li>
					<li>
						2025 May: <a href="2503.15661">UI-Vision</a> got accepted by <a href="index_27.html">ICML 2025</a>.
					</li>
					<li>
						2025 May: Give a talk at <a href="index_28.html">Allen Institute for AI</a>, hold by <a href="index_23.html">Prof. Ranjay</a>.
					</li>
					<li>
						2025 May: Give a talk at <a href="index_25.html">CMU</a>, hold by <a href="~katef.html">Prof. Katerina</a>.
					</li>
					<li>
						2025 Apr: Give a talk at <a href="index_24.html">Together AI</a>, hold by <a href="index_31.html">Prof. James Zou</a>.
					</li>
					<li>
						2025 Apr: Served as <a style="color:#FA8072">Area Chair</a> of <a href="index_21.html">NeurIPS
							2025</a>.
					</li>
					<li>
						2025 Feb: <a href="2411.17465">ShowUI</a>, <a href="2503.09402">VLog</a>, <a href="index_26.html">RoICtrl</a>, <a href="MovieBench.html">MovieBench</a> got accepted by <a href="index_30.html">CVPR 2025</a>.
					</li>
					<li>
						2025 Jan: <a href="Show-o.html">Show-o</a> got accepted by <a href="2025.html">ICLR 2025</a>. Congrats to the team!
					</li>
					<li>
						2024 Dec: <a href="2411.17465">ShowUI</a> (<a style="color:#FA8072">Oral</a>)
						received <a style="color:#FA8072"><u>Outstanding Paper Award</u></a> by <a href="home_1.html">NeurIPS Open-World Agents
							workshop
							2024</a>.
					</li>
					<li>
						2024 Nov: Recognized as <a href="ProgramCommittee.html#top-reviewers" style="color:#FA8072;">NeurIPS 2024 Top Reviewers</a>.
					</li>
					<li>
						2024 Sept: <a href="VideoGUI.html">VideoGUI</a> (<a style="color:#FA8072">Spotlight</a>), <a href="2408.16730">VideoLLM-MoD</a> got accepted by <a href="index_21.html">NeurIPS 2024</a>.
					</li>
					<li>
						2024 Aug: <a href="3688865.3689482">AssistGPT</a> got
						accepted by
						<a href="index_29.html">HCMA@ACM MM 2024</a> as <a style="color:#FA8072"><u>Best
								Demo
								Paper</u></a>.
					</li>
					<li>
						2024 July: <a href="2407.21757">MovieSeq</a> got accepted by <a href="index_33.html">ECCV 2024</a>.
					</li>
					<li>
						2024 Jun: <a href="2206.01670">EgoVLP</a> received <a href="2022_2023.html" style="color:#FA8072;"><u>Egocentric
								Vision
								(EgoVis) Distinguished Paper Award</u></a>.
					</li>
					<li>
						2024 May: Recognized as <a href="1793616950314369239.html" style="color:#FA8072;">CVPR 2024 Outstanding Reviewers</a>.
					</li>
					<li>
						2024 Feb: <a href="papers%3BChen_VideoLLM-online_Online_Video_Large_Language_Model_for_Streaming_Video_CVPR_2024_paper.pdf">VideoLLM-online</a>,
						<a href="2312.01987">SparseFormer</a> got accepted by <a href="index_30.html">CVPR 2024</a>.
					</li>
					<li>
						2023 Sept: <a href="2305.13777">VisorGPT</a> got accepted by <a href="index_32.html">NeurIPS 2023</a>.
					</li>
					<li>
						2023 Aug: <a href="2206.01670">EgoVLP</a> received <a href="index_38.html" ,="" style="color:#FA8072;"><u>PREMIA Best Student Paper Award
								(Gold award)</u></a>.
					</li>
					<li>
						2023 July: <a href="2307.16715">UniVTG</a>, <a href="2307.05463">EgoVLPv2</a>, <a href="2305.20087">TL;DR</a> got accepted by <a href="index_34.html">ICCV 2023</a>.
					</li>
					<li>
						2023 Mar: <a href="Wang_All_in_One_Exploring_Unified_Video-Language_Pre-Training_CVPR_2023_paper.html">All-in-one</a>,
						<a href="Chen_Affordance_Grounding_From_Demonstration_Video_To_Target_Image_CVPR_2023_paper.html">Afformer</a>
						got accepted by <a href="index_35.html">CVPR 2023</a>.
					</li>
					<li>
						2022 Sept: <a href="2206.01670">EgoVLP</a> (<a style="color:#FA8072">Spotlight</a>) got accepted by <a href="index_32.html">NeurIPS
							2022</a>.
					</li>
					<li>
						2022 Aug: Joined <a href="showlab.html">Show Lab @ NUS</a> to start my
						Ph.D. journey!
					</li>
					<li>
						2022 Jun: <a href="2206.01670">EgoVLP</a> won <a style="color:#FA8072">Double
							Champions</a> of
						<a href="cvpr2022w-ego4d-epic.html">Joint 1st
							Ego4D and 10th EPIC Workshop, CVPR 2022</a>. <a href="double-champions-at-the-2022-ieee-cvf-computer-vision-and-pattern-recognition-cvpr-epic-kitchens-challenges-and-ego4d-challenges.html">[News]</a>
					</li>
				</ul>
			</div>

			<h2>Selected Publications <a href="citations_2.html">[Google
					Scholar]</a></h2>
			† indicates equal contribution. <u style="text-decoration-color: #6495ED;">Denotes student I mentored.</u>
			<table id="tbPublications" width="100%">
				<tbody>
					<tr>
						<td width="260">
							<img src="images/aui.png" width="240px" style="box-shadow: 4px 4px 8px #888">
						</td>
						<td><a href="2511.15567v1"><b>Computer-Use Agents as Judges for Generative User Interface</b></a><br>
							<u>Kevin QH. Lin†</u>, <u style="text-decoration-color: #6495ED;">Siyuan Hu†</u>, Linjie Li, Zhengyuan Yang, Lijuan Wang, Philip Torr, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<em>Preprint, </em>2025<br>
								[<a href="AUI.html">project</a>]
								[<a href="2511_1.15567v1">paper</a>]
								[<a href="AUI_2.html">code</a>]
								[<a href="AUI_1.html">demo</a>]
								<br>
						</p></td>
					</tr>
					<tr>
						<td width="260">
							<img src="images/vcode.png" width="240px" style="box-shadow: 4px 4px 8px #888">
						</td>
						<td><a href="2511.02778"><b>VCode: a Multimodal Coding Benchmark with SVG as Symbolic Visual Representation</b></a><br>
							<u>Kevin QH. Lin†</u>, <u style="text-decoration-color: #6495ED;">Yuhao Zheng†</u>, <u style="text-decoration-color: #6495ED;">Hangyu Ran†</u>, Dantong Zhu, Dongxing Mao, Linjie Li, Philip Torr, Alex JP. Wang.<br>
							<p style="margin-top:3px">
								<em>Preprint, </em>2025<br>
								[<a href="VCode.html">project</a>]
								[<a href="2511_1.02778">paper</a>]
								[<a href="VCode_2.html">code</a>]
								[<a href="VCode_1.html">demo</a>]
								<br>
						</p></td>
					</tr>
					<tr>
						<td width="260">
							<img src="images/paper2video.png" width="240px" style="box-shadow: 4px 4px 8px #888">
						</td>
						<td><a href="2510.05096"><b>Paper2Video: Automatic Video Generation from Scientific Papers</b></a><br>
							<u style="text-decoration-color: #6495ED;">Zeyu Zhu†</u>, <u>Kevin QH. Lin†</u>, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<em>Preprint, </em>2025<br>
								[<a href="Paper2Video.html">project</a>]
								[<a href="2510.05096">paper</a>]
								[<a href="Paper2Video_2.html">code</a>]
								[<a href="Paper2Video_1.html">dataset</a>]
								<br>
								<a style="color:#FA8072">1.8K github stars.</a>
						</p></td>
					</tr>
					<tr>
						<td width="260">
							<img src="images/code2video.png" width="240px" style="box-shadow: 4px 4px 8px #888">
						</td>
						<td><a href="2510.01174"><b>Code2Video: A Code-centric Paradigm for Educational Video Generation</b></a><br>
							<u style="text-decoration-color: #6495ED;">Yanzhe Chen†</u>, <u>Kevin QH. Lin†</u>, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<em>Preprint, </em>2025<br>
								[<a href="Code2Video.html">project</a>]
								[<a href="2510.01174">paper</a>]
								[<a href="Code2Video_1.html">code</a>]
								[<a href="MMMC.html">dataset</a>]
								[<a href="1974199353695941114.html">twitter</a>]								
								<br>
								<a style="color:#FA8072">1.2K github stars.</a>																
						</p></td>
					</tr>
					<tr>
						<td width="260">
							<img src="images/paper2poster.png" width="240px" style="box-shadow: 4px 4px 8px #888">
						</td>
						<td><a href="2505.21497"><b>Paper2Poster: Towards Multimodal Poster
									Automation from Scientific Papers</b></a><br>
							<u style="text-decoration-color: #6495ED;">Wei Pang†</u>, <u>Kevin QH. Lin†</u>, Xiangru
							Jian†, Xi He, Philip Torr.<br>
							<p style="margin-top:3px">
								<em>NeurIPS D&amp;B, </em>2025<br>
								<em>ICML MAS workshop, </em>2025. <a style="color:#FA8072">Oral</a><br>
								[<a href="2505.21497">paper</a>]
								[<a href="Paper2Poster_2.html">code</a>]
								[<a href="index.html">project</a>]
								[<a href="Paper2Poster_1.html">datasets</a>]
								[<a href="1927721150584390129.html">twitter</a>]
								[<a href="Paper2Poster.html">demo</a>]
								<br>
								<a style="color:#FA8072">2.9K github stars.</a>
						</p></td>
					</tr>
					<tr>
						<td width="260">
							<img src="images/ton.png" width="240px" style="box-shadow: 4px 4px 8px #888">
						</td>
						<td><a href="2505.16854"><b>Think or Not? Selective Reasoning via
									Reinforcement Learning for Vision-Language Models</b></a><br>
							<u style="text-decoration-color: #6495ED;">Jiaqi Wang†</u>, <u>Kevin QH. Lin†</u>, James
							Cheng, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<em>NeurIPS, </em>2025<br>
								[<a href="2505.16854">paper</a>]
								[<a href="TON.html">code</a>]
								[<a href="ton.html">huggingface</a>]
								<br>
						</p></td>
					</tr>
					<tr>
						<td width="260">
							<img src="images/videomind.jpg" width="240px" style="box-shadow: 4px 4px 8px #888">
						</td>
						<td><a href="2503.13444"><b>VideoMind: A Chain-of-LoRA Agent for Long
									Video Reasoning</b></a><br>
							Ye Liu†, <u>Kevin QH. Lin†</u>, Chang Wen Chen, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<em>Preprint, </em>2025<br>
								[<a href="2503.13444">paper</a>]
								[<a href="VideoMind.html">code</a>]
								[<a href="VideoMind-Dataset.html">dataset</a>]
								[<a href="index_36.html">project</a>]
								[<a href="VideoMind-2B.html">demo</a>]
								<br>
						</p></td>
					</tr>
					<tr>
						<td width="260">
							<img src="images/vla.png" width="240px" style="box-shadow: 4px 4px 8px #888">
						</td>
						<td><a href="2411_1.17465"><b>ShowUI: One Vision-Language-Action
									Model for GUI Visual Agent</b></a><br>
							<u>Kevin QH. Lin</u>, Linjie Li, Difei Gao, Zhengyuan Yang, Shiwei Wu, Zechen Bai, Stan WX.
							Lei, Lijuan Wang, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<em>CVPR, </em>2025<br>
								<em>NeurIPS OWA workshop, </em>2024. <a style="color:#FA8072">Oral</a><br>
								[<a href="2411.17465">paper</a>]
								[<a href="ShowUI_1.html">code</a>]
								[<a href="ShowUI-2B.html">huggingface</a>]
								[<a href="ShowUI-desktop.html">dataset</a>]
								[<a href="ShowUI.html">demo</a>]
								<br>
								<a style="color:#FA8072">Outstanding Paper Award, NeurIPS Open-World Agents Workshop
									2024.</a><br>
								<a style="color:#FA8072">The model has been downloaded for over 240,000 times. 1.6K github stars</a><br>
						</p></td>
					</tr>
					<tr>
						<td width="260">
							<img src="images/vlog.jpg" width="240px" style="box-shadow: 4px 4px 8px #888">
						</td>
						<td><a href="2503.09402"><b>VLog: Video-Language Models by Generative
									Retrieval of Narration Vocabulary</b></a><br>
							<u>Kevin QH. Lin</u>, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<!-- <em>NeurIPS OWA workshop, </em>2024. <a style="color:#FA8072">Oral</a><br> -->
								<em>CVPR, </em>2025<br>
								[<a href="2503.09402">paper</a>]
								[<a href="VLog.html">code</a>]
								<br>
						</p></td>
					</tr>
					<tr>
						<td width="260">
							<img src="images/videogui.jpg" width="240px" style="box-shadow: 4px 4px 8px #888">
						</td>
						<td><a href="VideoGUI.html"><b>VideoGUI: A Benchmark for GUI Automation
									from Instructional Videos</b></a><br>
							<u>Kevin QH. Lin</u>, Linjie Li, Difei Gao, Qinchen Wu, Mingyi Yan, Zhengyuan Yang, Lijuan
							Wang, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<em>NeurIPS D&amp;B, </em>2024. <a style="color:#FA8072">Spotlight</a><br>
								[<a href="preprint.pdf">paper</a>]
								[<a href="VideoGUI_1.html">code</a>]
								[<a href="VideoGUI.html">project</a>]
								<br>
								<!-- <a style="color:#FA8072">Can an agent recreate PowerPoint animation effects from instructional videos?</a><br> -->

						</p></td>
					</tr>
					<tr>
						<td width="260">
							<img src="images/movieseq.png" width="240px" style="box-shadow: 4px 4px 8px #888">
						</td>
						<td><a href="2407.21757"><b>Learning Video Context as Interleaved
									Multimodal Sequences </b></a> <br><u>Kevin QH. Lin</u>, Pengchuan Zhang, Difei Gao,
							Xide Xia, Joya Chen, Ziteng Gao, Jinheng Xie, Xuhong Xiao, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<em>ECCV, </em>2024<br>
								[<a href="2407.21757">paper</a>]
								[<a href="MovieSeq.html">code</a>]
								<br>
								<!-- <a style="color:#FA8072">Video in-context learning using interleaved sequences of images, videos, plots and dialogues.</a><br> -->
								<br>
						</p></td>
					</tr>
					<tr>
						<td width="260">
							<img src="images/univtg.jpg" width="240px" height="120px" style="box-shadow: 4px 4px 8px #888">
						</td>
						<td><a href="2307.16715"><b>UniVTG: Towards Unified Video-Language
									Temporal Grounding</b></a> <br>
							<u>Kevin QH. Lin</u>, Pengchuan Zhang, Joya Chen, Shraman Pramanick, Difei Gao, Alex JP.
							Wang, Rui Yan, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<!-- <em>International Conference on Computer Vision (<b>ICCV</b>), </em>2023.<br> -->
								<em>ICCV, </em>2023<br>
								[<a href="2307_1.16715">paper</a>]
								[<a href="UniVTG_1.html">code</a>]
								[<a href="UniVTG.html">demo</a>]
								<br>
								<!-- <font color="#FF0000">The first video temporal grounding pretraining model, unifying diverse temporal annotations to power moment retrieval, highlight detection and video summarization.</font> -->
								<!-- <a style="color:#FA8072">The first video temporal grounding pretraining model, unifying diverse temporal labels.</a><br> -->
						</p></td>
					</tr>
					<tr>
						<td width="260">
							<img src="images/framework.jpeg" width="240px" height="120px" style="box-shadow: 4px 4px 8px #888">
						</td>
						<!-- <td><a href="https://arxiv.org/abs/2206.01670"><b>Egocentric Video-Language Pretraining</b></a> -->
						<td><a href="2206.01670"><b>Egocentric Video-Language Pretraining</b></a>
							<br>
							<u>Kevin QH. Lin</u>, Alex JP. Wang, M. Soldan, M. Wray, R. Yan, Eric ZC. Xu, D. Gao, R. Tu,
							W. Zhao, W. Kong, C. Cai, H. Wang, D. Damen, B. Ghanem, W. Liu, Mike Z. Shou.<br>
							<p style="margin-top:3px">
								<!-- <em>Neural Information Processing Systems (<b>NeurIPS</b>), </em>2022. <font
									color="#FF0000"><b>Spotlight (1.7%)</b></font><br> -->
								<em>NeurIPS, </em>2022. <a style="color:#FA8072">Spotlight (1.7%)</a><br>
								[<a href="2206_1.01670">paper</a>]
								[<a href="EgoVLP_1.html">code</a>]
								[<a href="EgoVLP.html">project</a>]
								[<a href="poster.pdf">poster</a>]
								[<a href="double-champions-at-the-2022-ieee-cvf-computer-vision-and-pattern-recognition-cvpr-epic-kitchens-challenges-and-ego4d-challenges_1.html">media</a>]
								<br>
								<!-- <font color="#FF0000">The first egocentric vision-language pretrained model.<br> -->
								<!-- <a style="color:#FA8072">The first egocentric vision-language pretrained model. </a><br> -->
								<a style="color:#FA8072">EgoVis Distinguished Paper Award &amp; PREMIA Best Student Paper
									Award 2023.</a><br>
								<a style="color:#FA8072">Double champions in Ego4D &amp; Epic-Kitchens CVPR 2022
									challenges.</a><br>
								<!-- Double champions in Ego4D & Epic-Kitchens CVPR 2022 challenges.</font> <a href=https://cde.nus.edu.sg/ece/news-detail/double-champions-at-the-2022-ieee-cvf-computer-vision-and-pattern-recognition-cvpr-epic-kitchens-challenges-and-ego4d-challenges/>[News]</a> -->
						</p></td>
					</tr>
					<tr></tr>
					<tr></tr>
				</tbody>
			</table>
			

			<h2>Honors</h2>
			<ul>
				<li>
					<div style="float:left; text-align:left">DAAD AINeT Fellowship</div>
					<div style="float:right; text-align:right">2025</div>
				</li>
				<li>
					<div style="float:left; text-align:left">CVPR Doctoral Consortium</div>
					<div style="float:right; text-align:right">2025</div>
				</li>
				<li>
					<div style="float:left; text-align:left">Outstanding Paper Award, NeurIPS Open-World Agents</div>
					<div style="float:right; text-align:right">2024</div>
				</li>
				<li>
					<div style="float:left; text-align:left">NeurIPS Top Reviewers</div>
					<div style="float:right; text-align:right">2024</div>
				</li>
				<li>
					<div style="float:left; text-align:left">Best Demo Paper Award, ACM Multimedia HCMA</div>
					<div style="float:right; text-align:right">2024</div>
				</li>
				<li>
					<div style="float:left; text-align:left">Egocentric Vision (EgoVis) Distinguished Paper Award</div>
					<div style="float:right; text-align:right">2024</div>
				</li>
				<li>
					<div style="float:left; text-align:left">CVPR Outstanding Reviewers (Top 2%)</div>
					<div style="float:right; text-align:right">2024</div>
				</li>
				<li>
					<div style="float:left; text-align:left">PREMIA Best Student Paper Awards, Gold Award</div>
					<div style="float:right; text-align:right">2023</div>
				</li>
				<li>
					<div style="float:left; text-align:left">NeurIPS Scholar Award</div>
					<div style="float:right; text-align:right">2022</div>
				</li>
				<li>
					<!-- <div style="float:left; text-align:left">Tencent Rhino-Bird Research Scholarship <a href=https://mp.weixin.qq.com/s/06e7m8twMen9DzrAE0raPA>[News]</a></div> -->
					<div style="float:left; text-align:left">Tencent Rhino-Bird Research Scholarship, Second Prize
					</div>
					<div style="float:right; text-align:right">2022</div>
				</li>
				<li>
					<div style="float:left; text-align:left">1st Place on Ego4D - Object State Change Classiﬁcation
						Challenge, CVPR</div>
					<div style="float:right; text-align:right">2022</div>
				</li>
				<li>
					<div style="float:left; text-align:left"></div>1st Place on EPIC-Kitchens - Multi-Instance Retrieval
					Challenge, CVPR<div style="float:right; text-align:right">2022</div>
				</li>
				<!--
	<li>
		<div style="float:left; text-align:left">2nd Place on Ego4D - Natural Language Queries Challenge, CVPR</div> <div style="float:right; text-align:right">2022</div>
	</li>
-->
				<!-- <li>
					<div style="float:left; text-align:left">AAAI Student Scholarship</div>
					<div style="float:right; text-align:right">2022</div>
				</li>
				<li>
					<div style="float:left; text-align:left">SIGIR Student Travel Grant</div>
					<div style="float:right; text-align:right">2021</div>
				</li> -->
				<!--	<li>
		<div style="float:left; text-align:left">Outstanding Graduate at SZU</div> <div style="float:right; text-align:right">2022, 2019</div>
	</li>
-->
				<li>
					<div style="float:left; text-align:left">Show Lab Annual Award</div>
					<div style="float:right; text-align:right">2022, 2024</div>
				</li>
				<li>
					<div style="float:left; text-align:left">China National Scholarship</div>
					<div style="float:right; text-align:right">2018, 2021</div>
				</li>
			</ul>


			<h2>Service</h2>
			<ul>
				<li>
					<p>Area Chair: NeurIPS 2025.
				</p></li>
				<li>
					<p>Workshop Organizer: <a href="omg.html">Open Multimodal Gathering @ NUS</a>;
						<a href="home_2.html">Multimodal Video Agent @ CVPR 25</a>.
				</p></li>
				<li>
					<!-- SDM -->
					<p>Conference Reviewer: CVPR (2024 Outstanding Reviewers), ICCV, ECCV, NeurIPS (2024 Top Reviewers),
						ICML, ICLR, etc.</p>
				</li>
				<li>
					<p>Journal Reviewer: TPAMI, IJCV, TMLR, TNNLS, TMM, etc.</p>
				</li>
				<li>
					<p>Teaching Assistant:
						<!-- 						<a href="https://nusmods.com/courses/EE6934/deep-learning-advanced">EE6934 Deep Learning</a>, 
						<a href="https://nusmods.com/courses/EE6733/advanced-topics-on-vision-and-machine-learning">EE6733 Advanced Topics on Vision and Machine Learning</a>,
						<a href="https://nusmods.com/courses/EE6733/advanced-topics-on-vision-and-machine-learning">EE4212 Computer Vision</a>	 -->
						<a href="deep-learning-advanced.html">EE6934</a>,
						<a href="advanced-topics-on-vision-and-machine-learning.html">EE6733</a>,
						<a href="advanced-topics-on-vision-and-machine-learning.html">EE4212</a>
					</p>
				</li>
				<li>
					<p>Co-organizer of <a href="index_37.html">The AI Talks.</a>
				</p></li>
			</ul>

			<br>
			<div align="center">
				<!-- <a href="https://info.flagcounter.com/yZuS"><img src="https://s05.flagcounter.com/count2/yZuS/bg_FFFFFF/txt_000000/border_CCCCCC/columns_2/maxflags_10/viewers_0/labels_0/pageviews_1/flags_0/percent_0/" alt="Flag Counter" border="0"></a></div> -->
				<!-- <a href="https://info.flagcounter.com/yZuS"><img src="https://s05.flagcounter.com/map/yZuS/size_s/txt_000000/border_CCCCCC/pageviews_1/viewers_0/flags_0/" alt="Flag Counter" border="0"></a></div> -->
				<a href="yZuS.html"><img src="flags_0" alt="Flag Counter" border="0"></a>
			</div>
			<br>

			<div align="center">

				
					<font color="gray">© Kevin</font>
				
			</div>

			<script>
				(function (i, s, o, g, r, a, m) {
					i['GoogleAnalyticsObject'] = r; i[r] = i[r] || function () {
						(i[r].q = i[r].q || []).push(arguments)
					}, i[r].l = 1 * new Date(); a = s.createElement(o),
						m = s.getElementsByTagName(o)[0]; a.async = 1; a.src = g; m.parentNode.insertBefore(a, m)
				})(window, document, 'script', 'https://www.google-analytics.com/analytics.js', 'ga');

				ga('create', 'UA-88615920-1', 'auto');
				ga('send', 'pageview');
			</script>
		</div>
	</div>



</body></html>